# FocusFlow

An AI-powered education platform for students with **ADHD**, featuring a carbon-aware RAG architecture that minimizes compute waste while delivering high-quality, personalized explanations.

![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)
![License](https://img.shields.io/badge/license-Unspecified-lightgrey.svg)

> **Core Principle:** *"The greenest token is the one never generated."*

---

## What Problem Does This Solve?

Traditional e-learning platforms deliver one-size-fits-all content that overwhelms ADHD learners. Generic LLM assistants hallucinate or drift from course materials, and large cloud models waste energy generating verbose responses no one reads.

**FocusFlow addresses this by:**

- Grounding all answers in your actual course content (DB + PDFs)
- Using efficient local inference (Qwen3:4B via Ollama) instead of large cloud models
- Maximizing retrieval precision to minimize downstream token generation
- Tracking learner attention via eye/mouse tracking for future adaptive response shaping
- Displaying carbon savings to build awareness of AI compute costs

---

## Key Features

| Feature | Status | Description |
|---------|--------|-------------|
| **Hybrid Retrieval** | âœ… Implemented | Dense (FAISS) + Sparse (BM25) with RRF fusion |
| **Query Rewriting** | âœ… Implemented | LLM-based query expansion for better recall |
| **Cross-Encoder Reranking** | âœ… Implemented | Prunes to top-N high-signal chunks |
| **Context Compression** | âœ… Implemented | Extractive compression maximizes info density |
| **Local-First Inference** | âœ… Implemented | Qwen3:4B via Ollama â€” low watt-per-token |
| **Carbon Estimation** | âœ… Implemented | Client-side COâ‚‚ savings calculation + toast UI |
| **Eye/Mouse Tracking** | âœ… Implemented | Collects focus duration, tab switches, gaze data |
| **ADHD-Optimized Prompts** | âœ… Implemented | Step-by-step explanations with examples |
| **Attention-Adaptive Budgets** | ğŸ”œ Planned | Dynamic token limits based on attention score |
| **Grid-Aware Throttling** | ğŸ”œ Planned | Reduce generation during high-carbon periods |

---

## Tech Stack

- **Backend:** Python 3.10+, FastAPI, Uvicorn, SQLAlchemy
- **LLM:** Ollama (local) with Qwen3:4B (configurable)
- **Embeddings:** HuggingFace `all-MiniLM-L6-v2`
- **Vector Store:** FAISS (persisted to `faiss_index/`)
- **Sparse Retrieval:** BM25 via LangChain
- **Reranker:** CrossEncoder `ms-marco-MiniLM-L-6-v2`
- **Frontend:** Jinja2 templates + Tailwind CSS
- **Database:** SQLite (default) via SQLAlchemy

---

## Quick Start

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.ai) running locally with a model pulled:
  ```bash
  ollama pull qwen3:4b
  ```

### Installation

```bash
# Clone the repository
git clone https://github.com/your-org/focusflow.git
cd focusflow

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Run

```bash
python server.py
# Or with uvicorn directly:
uvicorn server:app --host 0.0.0.0 --port 8000 --reload
```

Open http://localhost:8000 in your browser.

### Minimal API Test

```bash
curl -X POST http://localhost:8000/api/rag/answer \
  -H "Content-Type: application/json" \
  -d '{"question": "What is cognitive load?"}'
```

---

## Project Structure

```
Focus/
â”œâ”€â”€ server.py              # FastAPI app entry point
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ models/
â”‚   â””â”€â”€ main.py            # RAG pipeline: retrieval, rerank, compression, LLM
â”œâ”€â”€ routers/
â”‚   â””â”€â”€ edu.py             # Course CRUD, eye tracking, AI evaluation endpoints
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ models.py          # SQLAlchemy models (Courses, User, EyeTrackingSession)
â”‚   â””â”€â”€ settings.py        # DB connection settings
â”œâ”€â”€ templates/             # Jinja2 HTML templates
â”œâ”€â”€ static/                # CSS, JS, images
â”œâ”€â”€ pdfs/                  # Drop PDFs here for ingestion
â””â”€â”€ faiss_index/           # Persisted FAISS vector index
```

---

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Homepage |
| `POST` | `/api/rag/answer` | Generate RAG answer `{"question": "..."}` |
| `GET` | `/edu/courses` | List all courses |
| `POST` | `/edu/courses` | Add a new course |
| `GET` | `/edu/courses/{id}` | Get course detail + RAG explanation |
| `POST` | `/edu/save-eye-tracking` | Save eye tracking session data |
| `GET` | `/edu/statistics` | User statistics dashboard |

---

## Configuration

Set via `.env` file or environment variables:

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama API endpoint |
| `OLLAMA_MODEL` | `qwen3:4b` | Model to use for generation |
| `OLLAMA_TIMEOUT` | `60` | Request timeout (seconds) |
| `OLLAMA_MAX_TOKENS` | `512` | Max tokens per response |
| `ENABLE_QUERY_REWRITE` | `true` | Enable query rewriting |
| `ENABLE_HYBRID_RETRIEVAL` | `true` | Enable dense + sparse retrieval |
| `ENABLE_RERANK` | `true` | Enable cross-encoder reranking |
| `ENABLE_COMPRESSION` | `true` | Enable context compression |
| `DENSE_K` | `8` | Number of dense retrieval results |
| `SPARSE_K` | `12` | Number of BM25 results |
| `RERANK_TOP_N` | `4` | Top N after reranking |
| `MAX_CONTEXT_CHARS` | `6000` | Max context size for compression |

---

## Architecture Deep Dive

### Carbon-Aware Adaptive RAG Pipeline

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Rewriting â”‚  â† LRU cached, intent normalization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Hybrid Retrieval           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  FAISS  â”‚    â”‚  BM25   â”‚     â”‚  â† Parallel execution
â”‚  â”‚ (dense) â”‚    â”‚(sparse) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚              â–¼                  â”‚
â”‚     RRF Fusion + Dedup          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cross-Encoder Reranking       â”‚  â† Cached, batched scoring
â”‚   (top-4 high-signal chunks)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Context Compression          â”‚  â† TF-IDF + query overlap
â”‚    (preserve definitions,       â”‚
â”‚     drop low-signal text)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Local LLM (Ollama Qwen3:4B)   â”‚  â† Low watt-per-token
â”‚   ADHD-optimized prompt         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Carbon Estimation (client)    â”‚  â† Toast notification
â”‚   baseline vs local savings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Each Layer Matters

| Layer | Token Impact | Engineering Choice |
|-------|--------------|-------------------|
| Query Rewrite | Improves recall â†’ fewer irrelevant chunks | LRU cache avoids repeated LLM calls |
| Hybrid Retrieval | Dense catches semantics, sparse catches exact terms | Parallel ThreadPoolExecutor |
| Reranking | Prunes mediocre context that degrades LLM quality | CrossEncoder with result caching |
| Compression | Maximizes info/token ratio | Sentence-level TF-IDF scoring |
| Local Inference | ~6x lower energy than cloud GPT-4 class models | Ollama with keep_alive for warmth |

### Carbon Estimation Formula

```
Carbon (gCOâ‚‚) = Energy (kWh) Ã— Carbon Intensity (gCOâ‚‚/kWh)

Baseline (cloud): 0.0009 kWh per 1k tokens
Local (Qwen3:4B): 0.00015 kWh per 1k tokens
Carbon intensity: 0.42 kg COâ‚‚/kWh (global average)

Savings = (Baseline - Local) Ã— tokens Ã— carbon_intensity Ã— 1000
```

---

## Eye Tracking & Attention Data

FocusFlow collects behavioral telemetry to understand learner engagement:

- **Focus duration** (good / warning / alert states)
- **Tab switches** (attention breaks)
- **Session duration**
- **Tracking mode** (webcam or mouse fallback)

Data is stored in `EyeTrackingSession` table for per-user analysis.

**Planned:** Use attention scores to dynamically adjust response length â€” short summaries for distracted users, detailed explanations for engaged users.

---

## AWS Architecture (Production Reference)

<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/f77b95ca-7147-4ce1-9d32-ac7978e17de0" />


**Key decisions:**
- ALB is the only public ingress; all compute in private subnets
- GPU nodes use spot instances with on-demand fallback
- SQS decouples heavy RAG jobs from API latency
- Multi-AZ for RDS and OpenSearch

---

## Roadmap

- [ ] **Attention-adaptive token budgets** â€” use eye tracking scores to control response length
- [ ] **Grid-aware throttling** â€” reduce generation during high carbon intensity periods
- [ ] **Dockerfile + docker-compose** â€” containerized deployment
- [ ] **CI/CD pipeline** â€” GitHub Actions for lint, test, deploy
- [ ] **Admin UI** â€” reindex FAISS, manage courses, view analytics
- [ ] **User authentication** â€” per-user progress and personalization
- [ ] **Benchmark suite** â€” measure latency, token efficiency, carbon savings

---

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/your-feature`
3. Make changes with tests where applicable
4. Submit a PR with a clear description

Please keep PRs focused and small. For large changes, open an issue first to discuss.

---

## License

License not yet specified. Contact maintainers before redistribution.

---

## Acknowledgments

- [LangChain](https://langchain.com) for RAG primitives
- [Ollama](https://ollama.ai) for local LLM inference
- [FAISS](https://github.com/facebookresearch/faiss) for vector search
- [Sentence Transformers](https://sbert.net) for embeddings and reranking
# Focus-DOGUSHACKATHON
